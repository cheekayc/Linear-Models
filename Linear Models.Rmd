---
title: "Linear Models"
author: "Lectured by Jeff Goldsmith"
date: "2022-11-10"
output: github_document
---

```{r setup}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)

library(tidyverse)
library(p8105.datasets)

set.seed(1) # to ensure reproducibility
```

**Check this [page](https://www.statology.org/t-test-linear-regression/) for simple explanation of linear regression.**

### Predictors in a linear model

* **Outcome** is **continuous**.

* **Continuous exposure** can be added to the model directly, but **categorical exposure** must create dummy variables.


## *Model Fitting*

* `lm` = linear models (continuous outcome)

* `glm` = generalized linear models (non-continuous outcome)

* Use ***broom package*** for output

Load and clean `nyc_airbnb` dataset.
```{r}
data("nyc_airbnb")

nyc_airbnb = 
  nyc_airbnb %>% 
  # make 10 stars rating to 5 stars.
  mutate(stars = review_scores_location / 2) %>% 
  rename(
    borough = neighbourhood_group,
    neighborhood = neighbourhood) %>% 
  filter(borough != "Staten Island") %>% # because Staten Island data is too small. Not meaningful to analyze that.
  select(price, stars, borough, neighborhood, room_type)
```

Fit a linear model to look at how **price** (outcome) changes with **rating** and **borough**.
```{r}
model = lm(price ~ stars + borough, data = nyc_airbnb)
# Regression equation: price = -70.41 + 31.99*stars + 40.50*Brooklyn + 90.25*Manhattan + 13.21*Queens

model 
# Up to this point, we only get the coefficients of each variable.

summary(model) 
# Summary will give us a lot of information, such as standard error, t-value, p-value, r-squared, etc.

# To output the results in a tidy way, use broom::tidy.
model %>% 
  broom::tidy() %>% 
  mutate(
    term = str_replace(term, "borough", "Borough: ")) %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 2)
```

In the example above, Bronx is the reference category according to alphabetical order.                                                      
What if we want to change the reference category?
```{r}
model_change_ref = 
  nyc_airbnb %>% 
  mutate(
    borough = fct_infreq(borough)) %>% # Now the most common category would be the reference group.
  lm(price ~ stars + borough, data = .)

model_change_ref %>% 
  broom::tidy() %>% 
  mutate(
    term = str_replace(term, "borough", "Borough: ")) %>% 
  select(term, estimate, p.value) %>% 
  knitr::kable(digits = 2)
```


#### `broom::glance`

* `broom::tidy` gives us estimate, std.error, statistics (t-distribution - correlation coefficient use t-distribution as well), and p-value.

* `broom::glance` gives us r-squared, adj.r-squared, sigma, statistic, p-value, df, logLik, AIC, BIC, deviance, df.residual, n_obs

```{r}
model %>% 
  broom::glance() %>%
  select(r.squared, p.value, AIC)
```
**AIC** tests how well a model fits the data...

## **Diagnostics**

The **fitted** (or predicted) values are the y-values that you would expect for the given x-values according to the built regression model
(or visually, the best-fitting straight regression line).

To use linear regression, the following assumptions must be met:                                                                     
1) **Linearity**: The relationship between the independent and dependent variables is linear. 
  * Check this assumption by examining a scatterplot of x and y.  
  
2) **Homoscedasticity**: The variance of residual is the same for any value of X.
  * Check this assumption by examining the scatterplot of “residuals versus fits”; the variance of the residuals should be the same across
  all values of the x-axis. If the plot shows a pattern (e.g., bowtie or megaphone shape), then variances are not consistent, and this
  assumption has not been met.
  
3) **Independence**: Observations are independent of each other (not repeated by the same person, don't live in same household, not siblings, etc.)
There is no relationship between the residuals and the predictors; in other words, is independent of errors. 
  * Check this assumption by examining a scatterplot of “residuals versus fits”; the correlation should be approximately 0. In other words, 
  there should not look like there is a relationship.
  
4) **Normality**: The residuals must be approximately normally distributed.
  * Check this assumption by examining a normal probability plot; the observations should be near the line. You can also examine a histogram 
  of the residuals; it should be approximately normally distributed.

```{r}
# Get residuals and look at them:
modelr::add_residuals(nyc_airbnb, model) %>% # in the () indicates which datsset and which model
  ggplot(aes(x = stars, y = resid)) +
  geom_point()

# We should see mean zero & constant variance.
# From the plot, it seems like stars 4-5 have more variances. So constant variance assumption is violated here.

nyc_airbnb %>% 
  modelr::add_residuals(model) %>% 
  ggplot(aes(x = borough, y = resid)) +
  geom_violin() +
  # zoom in a little bit..
  ylim(-250, 250)
# From the plot, we can see that the variances are not constant. Distribution seems skewed.
# Linear regression assumes constant residuals.. So this assumption is violated.
```


## Hypothesis Testing

#### One coefficient: Example (stars)
```{r}
model_change_ref %>% 
  broom::tidy()

model_null = lm(price ~ stars, data = nyc_airbnb)
model_alt = lm(price ~ stars + borough, data = nyc_airbnb)

anova(model_null, model_alt) %>% 
  broom::tidy()
# p-value for H_alt is super small, so we should put "borough" in our model.
```


## Interaction

In the airbnb data, we might think that star ratings and room type affects price differently in each borough. 
One way to allow this kind of effect modification is through interaction terms:

```{r}
model_int = 
  nyc_airbnb %>% 
  lm(price ~ stars + borough * room_type, data = .)

model_int %>% 
  broom::tidy() %>% 
  knitr::kable(digits = 3)
```
This works, but the output takes time to think through. What does the output mean? What is the effect of room type in each borough?

Alternatively, we can nest within boroughs and fit borough-specific models associating price with rating and room type:
The advantage of doing this is that it makes the interpretation easier.

Fit models by borough:
```{r}
# I can nest everything inside the airbnb dataset according to borough:
nest_df = 
  nyc_airbnb %>% 
    nest(df = -borough) %>% 
    mutate(
    # Create a new variable that map the linear regression function to each of the borough.
      models = map(.x = df, ~lm(price ~ stars + room_type, data = .x)),
    # Then, create another variable that shows the tidy results of the linear regression.
      results = map(models, broom::tidy)) %>% 
    pull(results)

# From the results, we can see that in Queens, private room is $69.25 less than an entire home/apt. 
# In Queens, shared room is $94.97 less than an entire home/apt. 
```

We can also unnest the "results":
```{r}
unnest = 
  nyc_airbnb %>% 
  nest(df = -borough) %>% 
  mutate(
    models = map(.x = df, ~lm(price ~ stars + room_type, data = .x)),
    results = map(models, broom::tidy)) %>% 
  select(borough, results) %>% 
  unnest(results)
```

After unnesting the results, we can pivot wider to make the table looks nicer.
```{r}
unnest %>% 
  select(borough, term, estimate) %>% 
  mutate(term = fct_inorder(term)) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  knitr::kable(digits = 3)
```

A quick double check:
```{r}
nyc_airbnb %>% 
  filter(borough == "Bronx") %>% 
  lm(price ~ stars + room_type, data = .) %>% 
  broom::tidy()
```
Yes!! It looks like everything looks neat!


## Binary Outcomes

Import, clean, and wrangle the *Washington Post* `50 cities homicide` data for analysis.
```{r}
baltimore_df = 
  read_csv("Data/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(
    resolved = as.numeric(disposition == "Closed by arrest"), # make a new variable "resolved", if "closed by arrest" then resolved = 1.
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White")) %>% # make the race variable a factor and move "White" to the front of all races.
  select(resolved, victim_age, victim_race, victim_sex)
```

Using these data, we can fit a logistic regression for the binary “resolved” outcome and victim demographics as predictors. 

```{r}
fit_logistic = 
  baltimore_df %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, data = . , family = binomial()) 

# Binomial distribution can be thought of as simply the probability of a SUCCESS or FAILURE outcome in an experiment or survey that is
# repeated multiple times. The binomial is a type of distribution that has two possible outcomes (the prefix “bi” means two, or twice).

# We use broom::tidy to make the output human readable:
fit_logistic %>% 
  broom::tidy() %>% 
  # create a new variable that shows the ORs
  mutate(OR = exp(estimate)) %>%
  select(term, log_OR = estimate, OR, p.value) %>% 
  knitr::kable(digits = 3)
```

Homicides in which the victim is Black are substantially less likely to be resolved that those in which the victim is white; 
for other races the effects are not significant, possible due to small sample sizes. Homicides in which the victim is male are 
significantly less like to be resolved than those in which the victim is female. The effect of age is statistically significant, 
but careful data inspections should be conducted before interpreting too deeply.
